# -*- coding: utf-8 -*-
"""Earthquake_forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H0qUTddliYFi3hlnxx7B5SpRRTj-byID
"""

from google.colab import files
uploaded = files.upload()
file_path = list(uploaded.keys())[0]
print("Uploaded:", file_path)

import pandas as pd
import numpy as np
df = pd.read_csv(file_path)
print("Shape:", df.shape)
df.head(5)

df.info()

df.describe()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

df.isnull().sum()

# Drop useless columns with >50% missing values
df = df.drop(columns=['dmin','horizontalError','depthError','magError'])

# Fill missing values with median
for col in ['nst','gap','rms','magNst']:
    df[col] = df[col].fillna(df[col].median())

df.head()

df.info()

df.describe()

# 1. Distribution of Earthquake Magnitudes
plt.figure(figsize=(8,5))
sns.histplot(df['mag'], bins=30, kde=True, color='skyblue')
plt.title("Distribution of Earthquake Magnitudes", fontsize=14)
plt.xlabel("Magnitude")
plt.ylabel("Frequency")
plt.show()

# 2. Depth vs Magnitude (Scatter plot)
plt.figure(figsize=(8,5))
sns.scatterplot(x='depth', y='mag', data=df, alpha=0.6)
plt.title("Depth vs Magnitude", fontsize=14)
plt.xlabel("Depth (km)")
plt.ylabel("Magnitude")
plt.show()

# 3. Top 10 Locations with Most Earthquakes
top_places = df['place'].value_counts().head(10)
plt.figure(figsize=(10,6))
sns.barplot(x=top_places.values, y=top_places.index, palette="viridis", hue=top_places.index, legend=False)
plt.title("Top 10 Locations with Most Earthquakes", fontsize=14)
plt.xlabel("Number of Earthquakes")
plt.ylabel("Location")
plt.show()

"""Feature Engineering - Create magnitude category

"""

def categorize_magnitude(mag):
    if mag < 4.0:
        return "Low"
    elif 4.0 <= mag < 6.0:
        return "Moderate"
    elif 6.0 <= mag < 7.0:
        return "Strong"
    elif 7.0 <= mag < 8.0:
        return "Major"
    else:
        return "Great"

df["mag_category"] = df["mag"].apply(categorize_magnitude)

# Check result
print(df[["mag", "mag_category"]].head(10))
print(df[["mag", "mag_category"]].tail(10))

"""Encoding columns"""

cat_cols = ["magType", "net", "place", "type", "status", "locationSource", "magSource", "mag_category"]

le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

# Check result
print(df[cat_cols].head(5))

"""Scaling numericals"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scale_cols = ["depth", "mag", "gap", "rms", "nst", "magNst"]

df[scale_cols] = scaler.fit_transform(df[scale_cols])

# âœ… Check result
print(df[scale_cols].head(5))
print(df[scale_cols].tail(5))

"""



Feature Selection"""

import seaborn as sns
import matplotlib.pyplot as plt

# Compute correlation matrix
corr = df[scale_cols].corr()

# Plot heatmap
plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.show()

X = df.drop(columns=['time', 'id', 'updated', 'mag_category']) # Dropping original time, id, updated, and target column
y = df['mag_category']
# Create a DataFrame to visualize feature importance
feature_importance_df = pd.DataFrame({
    'Features': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Features', data=feature_importance_df, palette="viridis", hue='Features', legend=False)
plt.title("Feature Importance for Earthquake Category Prediction", fontsize=16)
plt.xlabel("Importance Score", fontsize=12)
plt.ylabel("Features", fontsize=12)
plt.tight_layout()
plt.show()

print("\nTop 10 most important features:")
print(feature_importance_df.head(10))

"""Splitting data into training and testing sets"""

from sklearn.model_selection import train_test_split

# Select a smaller subset of the data (e.g., 50%)
df_subset = df.sample(frac=0.5, random_state=42)

# Select the top 3 features
X = df_subset[['mag', 'magNst', 'nst']]
y = df_subset['mag_category']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

df_subset.shape

"""Training Logistic Regression , Random forest , XGboast and KNN modesls



"""

# Initialize the Logistic Regression model
log_reg_model = LogisticRegression(random_state=42, max_iter=5000) # Increased max_iter for convergence

# Train the model
log_reg_model.fit(X_train, y_train)

print("Logistic Regression model trained successfully!")

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

print("Random Forest model trained successfully!")

xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), random_state=42)

# Train the model
xgb_model.fit(X_train, y_train)

print("XGBoost model trained successfully!")

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn_model.fit(X_train, y_train)

print("KNN model trained successfully!")

"""Evaluating All Models (Combined)"""

# Evaluate Logistic Regression
y_pred_lr = log_reg_model.predict(X_test)
print("Logistic Regression Evaluation ")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Classification Report:\n", classification_report(y_test, y_pred_lr,zero_division=1))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))

# Evaluate Random Forest
y_pred_rf = rf_model.predict(X_test)
print("\n Random Forest Evaluation")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf,zero_division=1))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

# Evaluate XGBoost
y_pred_xgb = xgb_model.predict(X_test)
print("\n XGBoost Evaluation ")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb,zero_division=1))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

# Evaluate KNN
y_pred_knn = knn_model.predict(X_test)
print("\n KNN Evaluation ")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn,zero_division=1))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))

""" Random Forest model to a file using joblib.
 RF has the highest accuracy along with the XgB model
 though i going with Random forest Moddel

"""

import joblib

# Choose a filename for your model
filename = 'Earthquake_forecasting.pkl'

# Save the model using joblib
joblib.dump(rf_model, filename)

print(f"Random Forest model saved to {filename}")

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import pandas as pd
# import numpy as np
# 
# # Load the trained model
# try:
#     model = joblib.load('Earthquake_forecasting.pkl')
# except FileNotFoundError:
#     st.error("Model file not found. Please make sure 'Earthquake_forecasting.pkl' is in the same directory as the app or provide the full path.")
#     st.stop()
# 
# st.title('Earthquake Category Prediction')
# 
# st.write("Enter the features to predict the earthquake category.")
# 
# # Add input fields for the features used in the model
# # Based on your feature selection, these are 'mag', 'magNst', and 'nst'
# mag = st.number_input('Magnitude (mag)', min_value=0.0, max_value=10.0, value=4.5)
# magNst = st.number_input('Magnitude Station Count (magNst)', min_value=0.0, value=10.0)
# nst = st.number_input('Station Count (nst)', min_value=0.0, value=20.0)
# 
# 
# # Create a button to trigger prediction
# if st.button('Predict Category'):
#     # Create a DataFrame with the input features
#     # The column names must match the features used during training
#     input_data = pd.DataFrame([[mag, magNst, nst]], columns=['mag', 'magNst', 'nst'])
# 
#     # Make prediction using the processed input data
#     prediction = model.predict(input_data)
# 
# 
#     # Map the predicted category back to the original labels
#     # Reconstructing mapping based on observations (0: Low, 1: Major, 2: Moderate, 3: Strong)
#     category_mapping = {0: 'Low', 1: 'Major', 2: 'Moderate', 3: 'Strong'} # Reconstructed mapping based on observations
# 
# 
#     predicted_category = category_mapping.get(prediction[0], 'Unknown')
# 
# 
#     st.success(f'Predicted Earthquake Category: {predicted_category}')



